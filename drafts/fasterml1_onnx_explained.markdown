# Faster ML 1: Recipes For Onnx Runtime

If you want to run a deep neural network fast or lean, you need ONNX Runtime. 

Open Neural Network eXchange (ONNX) [1.4] is an Intermediate Representation, which sits between the graph 

### Part 1: Exporting Models As Onnx

### Part 2: Building Inference Runtime


## Resources

#### 1. Onnx Docs 
1. https://becominghuman.ai/a-deep-dive-into-onnx-onnx-runtime-part-2-785b523e0cca
2. https://becominghuman.ai/a-deep-dive-into-onnx-onnx-runtime-part-1-874517c66ffc
3. https://github.com/microsoft/onnxruntime
4. https://onnxruntime.ai/docs/

#### 2. Intermediate Representations
1. https://link.springer.com/chapter/10.1007/978-3-030-05677-3_19
2. https://www.tensorflow.org/mlir
3. [DistIR: An Intermediate Representation and Simulator for Efficient Neural Network Distribution](https://arxiv.org/abs/2111.05426)

### Profiling Changes 
https://github.com/lutzroeder/Netron
